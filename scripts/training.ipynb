{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/thau04b/hghallab/comp/.venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/mnt/thau04b/hghallab/comp/.venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import open3d as o3d\n",
    "import sofar\n",
    "import sys, os, random\n",
    "sys.path.append('/autofs/thau04b/hghallab/comp/Final model')\n",
    "from utils_d import SonicomDatabase, DataConfiguration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "\n",
    "from HRTFNet_onefreq import MultiViewHRTFPredictionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY = \"WANDB API\"\n",
    "# wandb.login(key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement CFG class to handle configurations\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    sonicom_root = '/autofs/thau04b/hghallab/comp/Huawei/TechArena20241016/data/'\n",
    "    batch_size = 4\n",
    "    image_size = [1024, 1024]\n",
    "    downsampled_size = [256, 256]\n",
    "    task = 0\n",
    "    num_workers = 4\n",
    "    num_images = 19\n",
    "    num_epochs = 500000000000\n",
    "    learning_rate = 1e-3\n",
    "    output_size = 256  # Adjust based on HRTF size\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model_save_path = './best_onefreq_centered_model_unfrozen.pth'\n",
    "    # Wandb configurations\n",
    "    log_to_wandb = True\n",
    "    project_name = '2D_to_HRTF'\n",
    "    run_name = 'onefreq'\n",
    "    \n",
    "    precompute_sht = False\n",
    "    sht_order = 7\n",
    "    num_coeffs = (sht_order + 1) ** 2\n",
    "    \n",
    "    comments: \"\"\"MultiViewHRTFPredictionModel\n",
    "    A neural network model for predicting single-frequency Head-Related Transfer Functions (HRTFs) from multi-view point clouds of ears.\n",
    "    Components:\n",
    "        - PointNetFeatureExtractor: Extracts features from point clouds.\n",
    "            Input: [batch_size, 3, num_points]\n",
    "            Output: [batch_size, 1024, 1]\n",
    "        - ViewTransformer: Pools features from multiple views using transformer attention mechanism.\n",
    "            Input: [batch_size, num_views, 1024]\n",
    "            Output: [batch_size, 1024]\n",
    "        - Frequency Embedding: Encodes frequency information.\n",
    "            Input: [batch_size] (frequency indices)\n",
    "            Output: [batch_size, 16]\n",
    "        - MLP Regressor: Generates HRTF predictions for a single frequency.\n",
    "            Input: [batch_size, 2048 + 16] (combined ear features + frequency embedding)\n",
    "            Output: [batch_size, 793 * 2 * 2] (positions * ears * mag/phase)\n",
    "    Input:\n",
    "        - point_clouds: Tensor [batch_size, 2, num_views, num_points, 3]\n",
    "        - frequency: Tensor [batch_size] frequency indices\n",
    "    Output:\n",
    "        - HRTF predictions: [batch_size, 793, 2, 2] (positions, ears, magnitude/phase)\n",
    "        \n",
    "    The model processes both ears simultaneously and predicts magnitude and phase \n",
    "    for a specific frequency across all positions. The design allows efficient \n",
    "    feature extraction by processing point cloud features once and reusing them\n",
    "    across frequency predictions.\"\"\"\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhouda-ghallab\u001b[0m (\u001b[33mhouda222\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/thau04b/hghallab/comp/Final model/wandb/run-20241213_204645-70ds3zoo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/houda222/2D_to_HRTF/runs/70ds3zoo' target=\"_blank\">onefreq</a></strong> to <a href='https://wandb.ai/houda222/2D_to_HRTF' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/houda222/2D_to_HRTF' target=\"_blank\">https://wandb.ai/houda222/2D_to_HRTF</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/houda222/2D_to_HRTF/runs/70ds3zoo' target=\"_blank\">https://wandb.ai/houda222/2D_to_HRTF/runs/70ds3zoo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/houda222/2D_to_HRTF/runs/70ds3zoo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x71c7b091afe0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=cfg.project_name,\n",
    "    config=cfg,\n",
    "    name=cfg.run_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SEEDING DONE\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print('> SEEDING DONE')\n",
    "    \n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects:  90\n",
      "Total number of images:  3420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SHT coefficients: 100%|██████████| 90/90 [00:03<00:00, 29.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHT coefficients loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Pipeline Implementation\n",
    "\n",
    "# Create DataConfiguration\n",
    "config = DataConfiguration(\n",
    "    use_2d=True,\n",
    "    use_3d=True,\n",
    "    use_hrtf=True,\n",
    "    use_3d_head=False,\n",
    "    precompute_sht=cfg.precompute_sht\n",
    ")\n",
    "\n",
    "# Initialize dataset\n",
    "sd = SonicomDatabase(root_dir=cfg.sonicom_root, config=config, sht_order=cfg.sht_order, training_data=True, task_id=cfg.task)\n",
    "\n",
    "# Split the dataset into train and val sets\n",
    "train_size = int(1 * len(sd))\n",
    "val_size = len(sd) - train_size\n",
    "train_dataset, val_dataset = random_split(sd, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects:  10\n",
      "Total number of images:  380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SHT coefficients:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SHT coefficients: 100%|██████████| 10/10 [00:00<00:00, 31.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHT coefficients loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the test set using the dataset class\n",
    "test_sd = SonicomDatabase(root_dir=cfg.sonicom_root, config=config, sht_order=cfg.sht_order, training_data=False, task_id=0)\n",
    "test_dataloader = DataLoader(test_sd, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, pt, hrtf, _, _ = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 19, 30000, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for patient in pt:\n",
    "#     for ear in patient:\n",
    "#         for view in ear:\n",
    "#             print(view.shape)\n",
    "#             pcd = o3d.geometry.PointCloud()\n",
    "#             pcd.points = o3d.utility.Vector3dVector(view.cpu().numpy())\n",
    "#             o3d.io.write_point_cloud(f\"patient_.ply\", pcd)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsize_image(imgs, final_shape=(512, 512)):\n",
    "    \"\"\"\n",
    "    Downsizes the input image tensor to the specified final shape.\n",
    "    \n",
    "    Parameters:\n",
    "    imgs (torch.Tensor): Input image tensor with shape [B, E, I, C, H, W]\n",
    "    final_shape (tuple): Target shape (height, width)\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Resized tensor with shape [B, I, 2, final_H, final_W]\n",
    "    \"\"\"\n",
    "    B, E, I, C, H, W = imgs.shape\n",
    "    imgs = imgs.view(-1, H, W)  # Flatten to [B*E*I*C, H, W]\n",
    "    \n",
    "    # Add channel dimension for interpolate\n",
    "    imgs = imgs.unsqueeze(1)  # [B*E*I*C, 1, H, W]\n",
    "    \n",
    "    # Resize\n",
    "    imgs = F.interpolate(imgs, size=final_shape, mode='bilinear', align_corners=False)\n",
    "    \n",
    "    # Reshape back\n",
    "    imgs = imgs.squeeze(1)  # Remove the temporary channel dim\n",
    "    imgs = imgs.view(B, E, I, C, final_shape[0], final_shape[1])\n",
    "    \n",
    "    # Prepare final format: [B, I, 2, H, W]\n",
    "    imgs = imgs.squeeze(1)  # Remove ear dimension since we process one ear at a time\n",
    "    imgs = imgs.squeeze(2)  # Remove the extra channel dimension\n",
    "    \n",
    "    return imgs\n",
    "\n",
    "def normalize_image(image):\n",
    "    return image.float() / 255.0\n",
    "\n",
    "\n",
    "def concatenate_images_and_depth_maps(images, depth_maps):\n",
    "    return torch.cat([images, depth_maps], dim=3)  # Concatenate along channel dimension\n",
    "\n",
    "def preprocess_data(images, depth_maps):\n",
    "    images = normalize_image(images)\n",
    "    inputs = torch.cat([images, depth_maps], dim=3) \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_points(point_data):\n",
    "    points = point_data[:, :, :3]  # Shape: [batch_size, num_points, 3]\n",
    "    return points.permute(0, 2, 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_hrtf(hrtf_mag_phase):\n",
    "    # Split the real and imaginary parts\n",
    "    hrtf_magnitude, hrtf_phase = torch.chunk(hrtf_mag_phase, 2, dim=-1)\n",
    "    # Combine them to form the complex hrtf\n",
    "    hrtf = hrtf_magnitude * torch.exp(1j * hrtf_phase)\n",
    "    return hrtf\n",
    "\n",
    "def hrtf2magnitudephase(hrtf):\n",
    "    hrtf_magnitude = torch.abs(hrtf)\n",
    "    hrtf_phase = torch.angle(hrtf)\n",
    "    return torch.cat([hrtf_magnitude, hrtf_phase], dim=-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_hrtf_from_magnitude(magnitude):\n",
    "    magnitude = magnitude.to(dtype=torch.float32)\n",
    "    # Add epsilon to avoid log(0) and clamp negative values\n",
    "    epsilon = 1e-8\n",
    "    magnitude = magnitude.clamp(min=epsilon)\n",
    "    log_magnitude = torch.log(magnitude)\n",
    "\n",
    "    if torch.isnan(log_magnitude).any():\n",
    "        print(\"NaNs detected after logarithm\")\n",
    "        return None  # Exit or handle appropriately\n",
    "\n",
    "    # Compute FFT of log magnitude\n",
    "    fft_log_magnitude = torch.fft.fft(log_magnitude)\n",
    "    N = log_magnitude.shape[-1]\n",
    "    h = torch.zeros(N, device=magnitude.device, dtype=fft_log_magnitude.dtype)\n",
    "    if N % 2 == 0:\n",
    "        h[0] = h[N // 2] = 1\n",
    "        h[1:N // 2] = 2\n",
    "    else:\n",
    "        h[0] = 1\n",
    "        h[1:(N + 1) // 2] = 2\n",
    "    fft_log_magnitude *= h\n",
    "    analytic_signal = torch.fft.ifft(fft_log_magnitude)\n",
    "    phase = -analytic_signal.imag\n",
    "    complex_hrtf = magnitude * torch.exp(1j * phase)\n",
    "\n",
    "    return complex_hrtf\n",
    "\n",
    "def reconstruct_complex_hrtf(hrtf_magnitude_phase):\n",
    "    hrtf_magnitude, hrtf_phase = torch.chunk(hrtf_magnitude_phase, 2, dim=-1)\n",
    "    hrtf = hrtf_magnitude * torch.exp(1j * hrtf_phase)\n",
    "    return hrtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unconcatenate_point_clouds(combined_point_cloud):\n",
    "    # Assuming the combined point cloud is concatenated along axis 1\n",
    "    num_points = combined_point_cloud.shape[1] // 2\n",
    "    left_point_cloud = combined_point_cloud[:, :num_points]\n",
    "    right_point_cloud = combined_point_cloud[:, num_points:]\n",
    "    return transform_points(left_point_cloud).to(cfg.device), transform_points(right_point_cloud).to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that data was loaded successfully by visualizing some samples\n",
    "%matplotlib inline\n",
    "\n",
    "def visualize_data(imgs=None, depth_imgs=None, pt=None, true_hrtf=None, predicted_hrtf=None, plot_whole_batch=False, sampling_rate=48000):\n",
    "\n",
    "    if imgs is not None:\n",
    "        # imgs shape: [batch_size, num_ears, num_images_per_ear, channels, H, W]\n",
    "        batch_size, num_ears, num_images_per_ear, channels, H, W = imgs.shape\n",
    "        indices = range(batch_size) if plot_whole_batch else [0]\n",
    "        for idx in indices:\n",
    "            for ear in range(num_ears):\n",
    "                fig, axes = plt.subplots(1, num_images_per_ear, figsize=(15, 5))\n",
    "                for img_idx in range(num_images_per_ear):\n",
    "                    img = imgs[idx, ear, img_idx].cpu().numpy()\n",
    "                    if channels == 1:\n",
    "                        img = img[0]  # Extract the single channel\n",
    "                    else:\n",
    "                        img = np.transpose(img, (1, 2, 0))  # Convert to HxWxC\n",
    "                    axes[img_idx].imshow(img, cmap='gray' if channels == 1 else None)\n",
    "                    axes[img_idx].axis('off')\n",
    "                plt.suptitle(f'Sample {idx+1}, Ear {ear+1}')\n",
    "                plt.show()\n",
    "\n",
    "    if depth_imgs is not None:\n",
    "        # depth_imgs shape: [batch_size, num_ears, num_images_per_ear, channels, H, W]\n",
    "        batch_size, num_ears, num_images_per_ear, channels, H, W = depth_imgs.shape\n",
    "        indices = range(batch_size) if plot_whole_batch else [0]\n",
    "        for idx in indices:\n",
    "            for ear in range(num_ears):\n",
    "                fig, axes = plt.subplots(1, num_images_per_ear, figsize=(15, 5))\n",
    "                for img_idx in range(num_images_per_ear):\n",
    "                    img = depth_imgs[idx, ear, img_idx].cpu().numpy()\n",
    "                    if channels == 1:\n",
    "                        img = img[0]  # Extract the single channel\n",
    "                    else:\n",
    "                        img = np.transpose(img, (1, 2, 0))  # Convert to HxWxC\n",
    "                    axes[img_idx].imshow(img, cmap='plasma')\n",
    "                    axes[img_idx].axis('off')\n",
    "                plt.suptitle(f'Depth Sample {idx+1}, Ear {ear+1}')\n",
    "                plt.show()\n",
    "    \n",
    "    if pt is not None:\n",
    "        # Split the point clouds for left and right ears\n",
    "        pt1, pt2 = unconcatenate_point_clouds(pt)\n",
    "        \n",
    "        # Create a figure with two subplots side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), subplot_kw={'projection': '3d'})\n",
    "        \n",
    "        # Plot left ear\n",
    "        point_cloud1 = pt1[0].permute(1, 0).detach().cpu().numpy()\n",
    "        indices1 = np.random.choice(point_cloud1.shape[0], size=10000, replace=False)\n",
    "        sampled_points1 = point_cloud1[indices1]\n",
    "        ax1.scatter(sampled_points1[:, 0], sampled_points1[:, 1], sampled_points1[:, 2], s=1)\n",
    "        ax1.view_init(elev=90, azim=180)\n",
    "        ax1.set_title('Left Ear')\n",
    "        \n",
    "        # Plot right ear\n",
    "        point_cloud2 = pt2[0].permute(1, 0).detach().cpu().numpy()\n",
    "        indices2 = np.random.choice(point_cloud2.shape[0], size=10000, replace=False)\n",
    "        sampled_points2 = point_cloud2[indices2]\n",
    "        ax2.scatter(sampled_points2[:, 0], sampled_points2[:, 1], sampled_points2[:, 2], s=1)\n",
    "        ax2.view_init(elev=90, azim=0)\n",
    "        ax2.set_title('Right Ear')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # HRTF Visualization\n",
    "    if true_hrtf is not None or predicted_hrtf is not None:\n",
    "        batch_size = (\n",
    "            true_hrtf.shape[0] if true_hrtf is not None else predicted_hrtf.shape[0]\n",
    "        )\n",
    "        indices = range(batch_size) if plot_whole_batch else [0]\n",
    "\n",
    "        # Iterate over batch indices\n",
    "        for idx in indices:\n",
    "            # Determine HRTF size\n",
    "            if true_hrtf is not None:\n",
    "                hrtf_size = true_hrtf.shape[-1] // 2\n",
    "            else:\n",
    "                hrtf_size = predicted_hrtf.shape[-1] // 2\n",
    "\n",
    "            # Frequency axis\n",
    "            freq_axis = np.linspace(0, sampling_rate / 2, hrtf_size)\n",
    "\n",
    "            # Prepare figures for each ear\n",
    "            for ear_index in range(2):  # 0 for left ear, 1 for right ear\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "                if true_hrtf is not None:\n",
    "                    hrtf_true = true_hrtf[idx].cpu().numpy()  # Shape: [positions, ears, total_bins]\n",
    "                    # Split real and imaginary parts\n",
    "                    hrtf_true_real = hrtf_true[:, ear_index, :hrtf_size]\n",
    "                    hrtf_true_imag = hrtf_true[:, ear_index, hrtf_size:]\n",
    "                    # Construct complex HRTF\n",
    "                    hrtf_true_complex = hrtf_true_real + 1j * hrtf_true_imag\n",
    "                    # Compute magnitude in dB\n",
    "                    magnitude_true = 20 * np.log10(np.abs(hrtf_true_complex) + 1e-6)\n",
    "                    # Average over positions\n",
    "                    magnitude_true_mean = np.mean(magnitude_true, axis=0)  # Shape: [hrtf_size]\n",
    "\n",
    "                    # Plot true HRTF\n",
    "                    ax.plot(freq_axis, magnitude_true_mean, label='True HRTF')\n",
    "\n",
    "                if predicted_hrtf is not None:\n",
    "                    hrtf_pred = predicted_hrtf[idx].cpu().numpy()\n",
    "                    hrtf_pred_real = hrtf_pred[:, ear_index, :hrtf_size]\n",
    "                    hrtf_pred_imag = hrtf_pred[:, ear_index, hrtf_size:]\n",
    "                    hrtf_pred_complex = hrtf_pred_real + 1j * hrtf_pred_imag\n",
    "                    magnitude_pred = 20 * np.log10(np.abs(hrtf_pred_complex) + 1e-6)\n",
    "                    # Average over positions\n",
    "                    magnitude_pred_mean = np.mean(magnitude_pred, axis=0)\n",
    "\n",
    "                    # Plot predicted HRTF\n",
    "                    ax.plot(\n",
    "                        freq_axis,\n",
    "                        magnitude_pred_mean,\n",
    "                        label='Predicted HRTF',\n",
    "                        linestyle='--',\n",
    "                    )\n",
    "\n",
    "                ax.set_title(f'Sample {idx + 1} Ear {ear_index + 1} HRTF Magnitude Response')\n",
    "                ax.set_xlabel('Frequency (Hz)')\n",
    "                ax.set_ylabel('Magnitude (dB)')\n",
    "                ax.legend()\n",
    "                ax.grid(True)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (images, depth_maps, pt) in enumerate(train_dataloader):\n",
    "#     print(f\"Sample {i+1}:\")\n",
    "#     print(f\"Images shape: {images.shape}\")\n",
    "#     print(f\"Depth maps shape: {depth_maps.shape}\")\n",
    "#     print(f\"Point cloud shape: {pt.shape}\")\n",
    "#     print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:70ds3zoo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">onefreq</strong> at: <a href='https://wandb.ai/houda222/2D_to_HRTF/runs/70ds3zoo' target=\"_blank\">https://wandb.ai/houda222/2D_to_HRTF/runs/70ds3zoo</a><br/> View project at: <a href='https://wandb.ai/houda222/2D_to_HRTF' target=\"_blank\">https://wandb.ai/houda222/2D_to_HRTF</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241213_204645-70ds3zoo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:70ds3zoo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/thau04b/hghallab/comp/Final model/wandb/run-20241213_204658-4p9k1p9c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/houda222/2D_to_HRTF/runs/4p9k1p9c' target=\"_blank\">onefreq</a></strong> to <a href='https://wandb.ai/houda222/2D_to_HRTF' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/houda222/2D_to_HRTF' target=\"_blank\">https://wandb.ai/houda222/2D_to_HRTF</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/houda222/2D_to_HRTF/runs/4p9k1p9c' target=\"_blank\">https://wandb.ai/houda222/2D_to_HRTF/runs/4p9k1p9c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/houda222/2D_to_HRTF/runs/4p9k1p9c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x71c928d013c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize WandB\n",
    "wandb.init(project=cfg.project_name, name=cfg.run_name, config=vars(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SEEDING DONE\n"
     ]
    }
   ],
   "source": [
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/thau04b/hghallab/comp/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MultiViewHRTFPredictionModel().to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,680,884 trainable parameters.\n",
      "The feature extractor has 807,744 trainable parameters.\n",
      "The regressor has 5,365,860 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "print(f'The model has {num_params:,} trainable parameters.')\n",
    "\n",
    "num_params = count_parameters(model.feat)\n",
    "print(f'The feature extractor has {num_params:,} trainable parameters.')\n",
    "\n",
    "num_params = count_parameters(model.regression)\n",
    "print(f'The regressor has {num_params:,} trainable parameters.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def manage_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redefine MSD as a loss function\n",
    "class MeanSpectralDistortion_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.avg_hrir = sofar.read_sofa(\"/autofs/thau04b/hghallab/comp/Huawei/TechArena20241016/data/Average_HRTFs.sofa\", verbose=False)\n",
    "        self.source_positions = self.avg_hrir.SourcePosition\n",
    "        \n",
    "        # Convert indices and weights to tensors and register as buffers\n",
    "        elevation_index = torch.tensor(self._get_elevation_index(), device=device)\n",
    "        weights = torch.tensor(self._get_weights(), device=device)\n",
    "        \n",
    "        # Register as buffers so they move to GPU with the model\n",
    "        self.register_buffer('elevation_index', elevation_index)\n",
    "        self.register_buffer('weights', weights)\n",
    "\n",
    "    def forward(self, hrtf_ground_truth: torch.Tensor, hrtf_predicted: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hrtf_ground_truth: [batch_size, positions, channels, frequencies]\n",
    "            hrtf_predicted: [batch_size, positions, channels, frequencies]\n",
    "        \"\"\"\n",
    "        # Select elevations for batch\n",
    "        gt_selected = torch.index_select(hrtf_ground_truth, 1, self.elevation_index)\n",
    "        pred_selected = torch.index_select(hrtf_predicted, 1, self.elevation_index)\n",
    "        \n",
    "        # Compute magnitude difference\n",
    "        magnitude_diff = gt_selected.abs() - pred_selected.abs()\n",
    "        \n",
    "        # Apply weights (broadcasting over batch, positions, channels)\n",
    "        weighted_diff = self.weights.view(1, 1, 1, -1) * magnitude_diff\n",
    "        \n",
    "        # Compute mean squared error\n",
    "        weighted_error = (weighted_diff ** 2).mean()\n",
    "        \n",
    "        # Convert to dB\n",
    "        return 10 * torch.log10(weighted_error + 1e-8)\n",
    "    \n",
    "    def _get_weights(self):\n",
    "        \"\"\"\n",
    "        This function load the weights which are used when you calculate the spectral distortion/ baseline predictions\n",
    "        weights were calculated based on the paper \"Looking for a relevant similarity criterion fo HRTF clustering: a comparative study - Rozenn Nicol\".\n",
    "\n",
    "        Returns:\n",
    "               normalized_weights: torch.tensor\n",
    "\n",
    "        \"\"\"\n",
    "        # Generate a list of frequencies up to 24 kHz\n",
    "        frequencies_Hz = torch.linspace(0, 24000, 129)  # 129 points between 0 Hz and 24 kHz\n",
    "        frequencies_kHz = frequencies_Hz / 1000\n",
    "        inv_cb = 1 / (25 + 75 * (1 + 1.4 * frequencies_kHz**2) ** 0.69)  # inverse of delta (critical bandwidth)\n",
    "        a0 = sum(inv_cb)\n",
    "        normalized_weights = inv_cb / a0\n",
    "        return normalized_weights\n",
    "\n",
    "    def _get_elevation_index(self):\n",
    "        \"\"\"\n",
    "        Helper function to get elevation indexes.\n",
    "        Args:\n",
    "            You can change the elevation range as required. We will use the elvation range between -30 to 30\n",
    "            Returns:\n",
    "             all index for the elevation range\"\"\"\n",
    "        # this function gives the index of the directions for which you need to evaluate your results.\n",
    "\n",
    "        azimuths = self.source_positions[:, 0]\n",
    "        elevations = self.source_positions[:, 1]\n",
    "\n",
    "        # Define the elevation range\n",
    "        elevation_min = -30\n",
    "        elevation_max = 30\n",
    "        # Find the indices for the specific elevation range\n",
    "        elevation_indices = np.where((elevations >= elevation_min) & (elevations <= elevation_max))[0]\n",
    "\n",
    "        # Ensure that elevation_indices is a NumPy array of integers\n",
    "        return np.array(elevation_indices, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97738/267431367.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint)\n"
     ]
    }
   ],
   "source": [
    "checkpoint =\"/mnt/thau04b/hghallab/comp/Final model/best_onefreq_centered_model.pth\"\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model.load_state_dict(checkpoint, strict=True)\n",
    "for param in model.feat.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97738/1647559881.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weights = torch.tensor(self._get_weights(), device=device)\n"
     ]
    }
   ],
   "source": [
    "criterion = MeanSpectralDistortion_loss()\n",
    "mse = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=cfg.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses\n",
    "training_losses = []\n",
    "training_metrics = []\n",
    "validation_losses = []\n",
    "validation_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a tensor to accumulate the sum of HRTFs\n",
    "# hrtf_sum_real = torch.zeros((793, 2, 129), device=cfg.device)\n",
    "# hrtf_sum_imag = torch.zeros((793, 2, 129), device=cfg.device)\n",
    "# num_examples = 0\n",
    "\n",
    "# # Iterate through the train loader\n",
    "# for _, _, pts, hrtf in tqdm.tqdm(train_dataloader, desc=\"Calculating Mean HRTF\"):\n",
    "#     hrtf = hrtf.to(cfg.device)\n",
    "#     hrtf_sum_real += hrtf.real.sum(dim=0)\n",
    "#     hrtf_sum_imag += hrtf.imag.sum(dim=0)\n",
    "#     num_examples += hrtf.size(0)\n",
    "\n",
    "# # Calculate the mean HRTF\n",
    "# mean_hrtf_real = hrtf_sum_real / num_examples\n",
    "# mean_hrtf_imag = hrtf_sum_imag / num_examples\n",
    "\n",
    "# mean_hrtf = mean_hrtf_real + 1j * mean_hrtf_imag\n",
    "# mean_hrtf = mean_hrtf.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# # Save the mean HRTF to a file\n",
    "# mean_hrtf_path = os.path.join(cfg.sonicom_root, 'mean_hrtf.pt')\n",
    "# torch.save(mean_hrtf, mean_hrtf_path)\n",
    "\n",
    "# print(\"Mean HRTF calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean HRTF loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97738/2277146204.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mean_hrtf = torch.load(mean_hrtf_path).to(CFG.device)\n"
     ]
    }
   ],
   "source": [
    "mean_hrtf_path = os.path.join(cfg.sonicom_root, 'mean_hrtf.pt')\n",
    "mean_hrtf = torch.load(mean_hrtf_path).to(CFG.device)\n",
    "print(\"Mean HRTF loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, dataloader, optimizer, criterion, metric_criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    for _, _, pts, hrtf, _, _ in tqdm.tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        # Process depth maps\n",
    "        pts = pts.to(device)  # [batch_size, num_ears, num_views, H, W]\n",
    "        hrtf = hrtf.to(device) # [batch_size, 793, 2, 129*2]\n",
    "        batch_size = hrtf.shape[0]\n",
    "        num_freqs = hrtf.shape[-1]\n",
    "        \n",
    "        hrtf_magnitude_phase = hrtf2magnitudephase(hrtf)\n",
    "        \n",
    "        all_predictions = []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_magnitude_phase = model(pts)\n",
    "        \n",
    "        output_complex = complex_hrtf(output_magnitude_phase)\n",
    "        \n",
    "        uncentered_output_complex = output_complex + mean_hrtf.expand(batch_size, -1, -1, -1)\n",
    "        uncentered_output_mag_phase = hrtf2magnitudephase(uncentered_output_complex)\n",
    "        \n",
    "        loss = criterion(uncentered_output_complex, hrtf)\n",
    "        metric_loss = metric_criterion(uncentered_output_mag_phase, hrtf_magnitude_phase)\n",
    "\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_metric += metric_loss.item()\n",
    "        \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_metric = running_metric / len(dataloader)\n",
    "    training_losses.append(avg_loss)\n",
    "    training_metrics.append(avg_metric)\n",
    "    \n",
    "    if cfg.log_to_wandb:\n",
    "        wandb.log({'Train Loss': avg_loss, 'Epoch': epoch+1})\n",
    "    \n",
    "    return avg_loss, avg_metric\n",
    "\n",
    "def evaluate(epoch, model, dataloader, criterion, metric_criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0  # Track losses for each option\n",
    "    running_metric = 0.0 # Track metrics for each option\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, _, pts, hrtf, _, _ in tqdm.tqdm(dataloader, desc=\"Validation\"):\n",
    "            pts = pts.to(device)  # [batch_size, num_ears, num_views, H, W]\n",
    "            hrtf = hrtf.to(device) # [batch_size, 793, 2, 129*2]\n",
    "            batch_size = hrtf.shape[0]\n",
    "            num_freqs = hrtf.shape[-1]\n",
    "            \n",
    "            hrtf_magnitude_phase = hrtf2magnitudephase(hrtf)\n",
    "            \n",
    "            output_magnitude_phase = model(pts)\n",
    "            \n",
    "            \n",
    "            output_complex = complex_hrtf(output_magnitude_phase)\n",
    "            \n",
    "            uncentered_output_complex = output_complex + mean_hrtf.expand(batch_size, -1, -1, -1)\n",
    "            uncentered_output_mag_phase = hrtf2magnitudephase(uncentered_output_complex)\n",
    "            \n",
    "            loss = criterion(uncentered_output_complex, hrtf)\n",
    "            metric_loss = metric_criterion(uncentered_output_mag_phase, hrtf_magnitude_phase)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            running_loss += loss.item()\n",
    "            running_metric += metric_loss.item()\n",
    "            \n",
    "    \n",
    "    # Calculate average losses for each option\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_metric = running_metric / num_batches\n",
    "    \n",
    "    validation_losses.append(avg_loss)\n",
    "    validation_metrics.append(avg_metric)\n",
    "    \n",
    "    if cfg.log_to_wandb:\n",
    "        wandb.log({\n",
    "            'Validation Loss': avg_loss,\n",
    "            'Validation Metric': avg_metric,\n",
    "            'Epoch': epoch+1\n",
    "        })\n",
    "    \n",
    "    return avg_loss, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 23/23 [01:27<00:00,  3.80s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500000000000], Train Loss: -62.4593, Val Loss: -62.8875\n",
      "Model saved at epoch 1 with validation loss -62.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 23/23 [01:24<00:00,  3.68s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/500000000000], Train Loss: -62.5828, Val Loss: -63.0615\n",
      "Model saved at epoch 2 with validation loss -63.0615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/500000000000], Train Loss: -62.5305, Val Loss: -63.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 23/23 [01:23<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/500000000000], Train Loss: -62.4784, Val Loss: -63.0619\n",
      "Model saved at epoch 4 with validation loss -63.0619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/500000000000], Train Loss: -62.5891, Val Loss: -63.0524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 23/23 [01:24<00:00,  3.68s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/500000000000], Train Loss: -62.6287, Val Loss: -63.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/500000000000], Train Loss: -62.5440, Val Loss: -63.0575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/500000000000], Train Loss: -62.5604, Val Loss: -63.0568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 23/23 [01:24<00:00,  3.67s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/500000000000], Train Loss: -62.5636, Val Loss: -63.0592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500000000000], Train Loss: -62.6208, Val Loss: -63.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 23/23 [01:24<00:00,  3.67s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/500000000000], Train Loss: -62.5827, Val Loss: -63.0635\n",
      "Model saved at epoch 11 with validation loss -63.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 23/23 [01:23<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/500000000000], Train Loss: -62.7061, Val Loss: -63.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/500000000000], Train Loss: -62.6402, Val Loss: -63.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 23/23 [01:24<00:00,  3.68s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/500000000000], Train Loss: -62.6489, Val Loss: -63.0618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 23/23 [01:24<00:00,  3.67s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/500000000000], Train Loss: -62.6648, Val Loss: -63.0598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 23/23 [01:24<00:00,  3.67s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/500000000000], Train Loss: -62.6368, Val Loss: -63.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 23/23 [01:24<00:00,  3.67s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/500000000000], Train Loss: -62.5885, Val Loss: -63.0557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/500000000000], Train Loss: -62.6596, Val Loss: -63.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/500000000000], Train Loss: -62.6676, Val Loss: -63.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/500000000000], Train Loss: -62.6701, Val Loss: -63.0574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 23/23 [01:24<00:00,  3.68s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/500000000000], Train Loss: -62.6404, Val Loss: -63.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/500000000000], Train Loss: -62.6818, Val Loss: -63.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/500000000000], Train Loss: -62.6075, Val Loss: -63.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/500000000000], Train Loss: -62.6202, Val Loss: -63.0618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/500000000000], Train Loss: -62.6264, Val Loss: -63.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 23/23 [01:24<00:00,  3.67s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/500000000000], Train Loss: -62.5711, Val Loss: -63.0608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|██████████| 23/23 [01:24<00:00,  3.69s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/500000000000], Train Loss: -62.6212, Val Loss: -63.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 23/23 [01:23<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/500000000000], Train Loss: -62.6226, Val Loss: -63.0570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 23/23 [01:23<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/500000000000], Train Loss: -62.6431, Val Loss: -63.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/500000000000], Train Loss: -62.6352, Val Loss: -63.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/500000000000], Train Loss: -62.6102, Val Loss: -63.0630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/500000000000], Train Loss: -62.6927, Val Loss: -63.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/500000000000], Train Loss: -62.5702, Val Loss: -63.0618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|██████████| 23/23 [01:23<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/500000000000], Train Loss: -62.6826, Val Loss: -63.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/500000000000], Train Loss: -62.6313, Val Loss: -63.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/500000000000], Train Loss: -62.6374, Val Loss: -63.0631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/500000000000], Train Loss: -62.6294, Val Loss: -63.0571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|██████████| 23/23 [01:23<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/500000000000], Train Loss: -62.5476, Val Loss: -63.0610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|██████████| 23/23 [01:24<00:00,  3.65s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/500000000000], Train Loss: -62.6124, Val Loss: -63.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|██████████| 23/23 [01:25<00:00,  3.70s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/500000000000], Train Loss: -62.6386, Val Loss: -63.0602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|██████████| 23/23 [01:25<00:00,  3.71s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/500000000000], Train Loss: -62.6492, Val Loss: -63.0618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/500000000000], Train Loss: -62.7050, Val Loss: -63.0592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/500000000000], Train Loss: -62.6642, Val Loss: -63.0587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44: 100%|██████████| 23/23 [01:23<00:00,  3.64s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/500000000000], Train Loss: -62.5821, Val Loss: -63.0584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:11<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/500000000000], Train Loss: -62.6029, Val Loss: -63.0621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46:  70%|██████▉   | 16/23 [00:59<00:25,  3.58s/it]"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    manage_memory()\n",
    "    train_loss, train_metric = train_one_epoch(epoch, model, train_dataloader, optimizer, criterion, mse, cfg.device)\n",
    "    manage_memory()\n",
    "    val_loss, val_metric = evaluate(epoch, model, test_dataloader, criterion, mse, cfg.device)\n",
    "    # manage_memory()\n",
    "    # test_loss, test_metric = evaluate(epoch, model, test_dataloader, criterion, mse, cfg.device)\n",
    "    print(f'Epoch [{epoch+1}/{cfg.num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    # Log train and validation loss to wandb\n",
    "    if cfg.log_to_wandb:\n",
    "        wandb.log({\n",
    "            \"train/loss_MSD\": train_loss,\n",
    "            \"val/loss_MSD\": val_loss,\n",
    "            \"train/MSE\": train_metric,\n",
    "            \"val/MSE\": val_metric,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "    # Save the model if validation loss has decreased\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), cfg.model_save_path)\n",
    "        print(f\"Model saved at epoch {epoch+1} with validation loss {best_val_loss:.4f}\")\n",
    "        # Save checkpoint to wandb\n",
    "        if cfg.log_to_wandb:\n",
    "            wandb.save(cfg.model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Epoch [58/500000000000], Train Loss: -62.5544, Val Loss: -62.8132\n",
    "Train Loss: -62.5104, Val Loss: -62.8134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch [10/500000000000], Train Loss: -62.5241, Val Loss: -62.8182\n",
    "Model saved at epoch 10 with validation loss -62.8182"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
